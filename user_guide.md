# 针对小样本的椎间盘退变分级系统设计

## 1\. 项目概述

本项目旨在利用筛选出的**稳健影像组学特征**，通过无监督机器学习，建立一个全新的、数据驱动且具备临床可解释性的椎间盘退变分级系统。

该系统不依赖于现有的临床分级标准（如Pfirrmann分级），而是首先通过探索数据本身的内在结构发现自然群组，然后利用临床数据（如疼痛评分）对这些群组进行验证和排序，从而形成与临床表现高度相关的全新分级标准。

本流程特别针对**小样本量**（几十至一百个病例）进行了优化，采用了稳健的算法以避免过拟合。

-----

## 2\. 步骤与原理

本项目的分析流程主要包括以下四个步骤：

### 2.1 步骤一：数据预处理与标准化

*   **目标**：对输入的**稳健特征集**进行清洗、分组和标准化。
*   **算法**：程序默认采用`sklearn.preprocessing.RobustScaler`，该方法对异常值不敏感，在小样本数据上表现更稳健。同时，也可在 `config.py` 文件中配置使用`StandardScaler` (z-score) 或 `MinMaxScaler`。
*   **数学原理**：
    对于一个特征向量 $\mathbf{x} = \{x_1, x_2, ..., x_n\}$，`RobustScaler`首先计算其四分位数。
    1.  **第一四分位数 (Q1)**：将排序后数据的前半部分再次取中位数得到的值（第25百分位数）。
    2.  **第二四分位数 (Q2)**：即数据的中位数。将特征向量 $\mathbf{x}$ 按升序排序后，位于最中间位置的值。
    3.  **第三四分位数 (Q3)**：将排序后数据的后半部分再次取中位数得到的值（第75百分位数）。

    对于向量 $\mathbf{x}$ 中的每个观测值 $x_i$，标准化的计算公式为：
    $$x_{scaled, i} = \frac{x_i - Q_2(\mathbf{x})}{Q_3(\mathbf{x}) - Q_1(\mathbf{x})}$$
    分母 $Q_3(\mathbf{x}) - Q_1(\mathbf{x})$ 被称为**四分位距**。由于中位数和IQR的计算不直接受数据两端极端值的影响，因此这种标准化方法对数据中的异常点具有很强的鲁棒性。

### 2.2 步骤二：降维

*   **目标**：在保留数据主要变异信息的同时，将经过多视角标准化后的高维特征空间压缩到低维空间（2维或3维），以便于可视化和聚类。
*   **算法**：**主成分分析 (PCA)**。
*   **数学原理**：
    PCA的目标是找到一组新的正交基（主成分），使得数据在这些基上的投影方差最大化。其计算过程如下：

    1.  **数据中心化**：
        假设我们有 $n$ 个样本和 $p$ 个特征，构成一个数据矩阵 $\mathbf{X}$ (大小为 $n \times p$)。首先，对每个特征（每一列）计算其均值，并将该列的所有值减去均值，得到中心化后的矩阵 $\mathbf{B}$。
        $$ B_{ij} = X_{ij} - \bar{x}_j $$
        其中 $\bar{x}_j$ 是第 $j$ 个特征的均值。

    2.  **计算协方差矩阵**：
        计算中心化矩阵 $\mathbf{B}$ 的协方差矩阵 $\mathbf{C}$ (大小为 $p \times p$)。
        $$ \mathbf{C} = \frac{1}{n-1} \mathbf{B}^T \mathbf{B} $$
        协方差矩阵的对角线元素表示每个特征自身的方差，非对角线元素表示特征之间的协方差。

    3.  **特征值分解**：
        对协方差矩阵 $\mathbf{C}$ 进行特征值分解，得到一组特征值 $\lambda_1, \lambda_2, ..., \lambda_p$ 和对应的特征向量 $\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_p$。
        $$ \mathbf{C}\mathbf{v}_j = \lambda_j\mathbf{v}_j $$
        每个特征向量 $\mathbf{v}_j$ 代表一个新的坐标轴方向，即一个主成分。特征值 $\lambda_j$ 的大小代表了数据在对应主成分方向上的方差大小。

    4.  **选择主成分并投影**：
        将特征值按从大到小的顺序排列 ($\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p$)。选择前 $k$ 个最大的特征值对应的特征向量，构成一个投影矩阵 $\mathbf{W}$ (大小为 $p \times k$)。
        $$ \mathbf{W} = [\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_k] $$
        最后，将中心化后的原始数据 $\mathbf{B}$ 投影到这个新的 $k$ 维空间中，得到降维后的数据 $\mathbf{Z}$ (大小为 $n \times k$)。
        $$ \mathbf{Z} = \mathbf{B} \mathbf{W} $$
        $\mathbf{Z}$ 的第一列（PC1）即是数据在方差最大方向上的投影，第二列（PC2）是在与PC1正交方向上方差次大的投影，以此类推。

### 2.3 步骤三：聚类分析与等级定义

*   **目标**：在降维后的低维空间中，识别出数据点的自然群组，并为每个群组定义一个有序且具有临床意义的退变等级。
*   **算法与流程**：
    1.  **层次聚类**：采用**凝聚型层次聚类 ward** 算法。
        *   **原理**:
            Ward法是一种特殊的凝聚型聚类方法，其核心思想是：在每一步合并中，选择合并后能使得所有簇的**总簇内方差增量最小**的两个簇。
            - **初始化**：每个数据点自成一簇。
            - **迭代合并**：在凝聚的每一步，算法都需要评估合并任意两个簇 $(C_r, C_s)$ 的“成本”。这个成本就是合并操作所导致的“总簇内误差平方和”的增加量，我们将其记为 $\Delta(C_r, C_s)$。算法会选择使这个成本最小的一对簇进行合并。
            - **决策**：在凝聚型层次聚类中，每一步都需要决定合并哪两个簇。Ward法的决策依据是**最小化所有簇的“总簇内误差平方和”的增量**。

              假设在算法的某一步，数据被划分为 $m$ 个簇 $\{C_1, C_2, ..., C_m\}$。

              1.  **定义簇内误差平方和 (WCSS)**：
                  对于任意一个簇 $C_k$，其WCSS定义为该簇内所有数据点到其质心（均值向量）$\boldsymbol{\mu}_k$ 的欧氏距离平方之和：
                  $$ \text{WCSS}(C_k) = \sum_{\mathbf{x}_i \in C_k} ||\mathbf{x}_i - \boldsymbol{\mu}_k||^2 $$
                  此时，整个数据集的总WCSS为所有簇的WCSS之和：
                  $$ \text{WCSS}_{\text{total}} = \sum_{k=1}^{m} \text{WCSS}(C_k) $$

              2.  **评估合并的“成本”**：
                  Ward法的目标是找到一对簇 $(C_r, C_s)$，将它们合并成新簇 $C_t = C_r \cup C_s$ 后，$\text{WCSS}_{\text{total}}$ 的**增加量**最小。合并前，与 $C_r$ 和 $C_s$ 相关的总WCSS贡献是 $\text{WCSS}(C_r) + \text{WCSS}(C_s)$。合并后，这两个簇被 $C_t$ 替代，其贡献变为 $\text{WCSS}(C_t)$。因此，这次合并导致的WCSS增加量（即“成本”）为：
                  $$ \Delta(C_r, C_s) = \text{WCSS}(C_t) - (\text{WCSS}(C_r) + \text{WCSS}(C_s)) $$

              3.  **Lance-Williams递推公式的Ward形式**：
                  可以证明，上述增加量 $\Delta(C_r, C_s)$ 等价于一个更易于计算的形式：
                  $$ \Delta(C_r, C_s) = \frac{n_r n_s}{n_r + n_s} ||\boldsymbol{\mu}_r - \boldsymbol{\mu}_s||^2 $$
                  其中，$n_r$ 和 $n_s$ 分别是簇 $C_r$ 和 $C_s$ 的样本数，$\boldsymbol{\mu}_r$ 和 $\boldsymbol{\mu}_s$ 是它们的质心。这个公式直观地表明，合并的“成本”与两个簇质心之间的距离的平方成正比，并由它们的样本大小进行加权。质心相距越远、或两个簇的大小越接近，合并的成本就越高。

              4.  **决策过程**：
                  在每一步迭代中，算法会为**所有**现存的簇对 $(C_i, C_j)$ 计算出它们的合并成本 $\Delta(C_i, C_j)$。然后，它会选择使这个成本值**最小**的一对簇 $(C_r, C_s)$ 进行合并。
                  $$ (C_r, C_s) = \arg\min_{i \neq j} \Delta(C_i, C_j) $$
                  这个过程不断重复，直到所有样本点都归于一个簇。因为该方法总是试图最小化方差的增加，所以它倾向于合并那些紧凑的、球形的簇，并最终生成大小相对均匀的聚类结果。

    2.  **等级排序**:
        *   **模式一：临床数据驱动排序**: 此模式的目标是利用外部的临床信息来验证和排序通过影像组学特征生成的聚类，其核心是**假设检验**。

            1.  **通过算术平均值排序**
   
                对于通过聚类算法得到的 $K$ 个簇 $\{C_1, C_2, ..., C_K\}$，我们计算每个簇内所有样本的临床指标（例如，疼痛评分 $y$）的算术平均值 $\bar{y}_k$：
                $$ \bar{y}_k = \frac{1}{N_k} \sum_{i \in C_k} y_i $$
                其中 $N_k$ 是簇 $C_k$ 中的样本数量。然后根据 $\bar{y}_k$ 的值对所有簇进行升序（或降序）排列，从而赋予每个簇一个有序的等级。

            2.  **假设检验**
   
                为了证明这种排序是有意义的，即影像学上的分组确实对应了临床表现的真实差异，我们需要进行统计检验。

                *   **零假设 ($H_0$) 与备择假设 ($H_1$)**:
                    *   $H_0$：所有 $K$ 个簇的临床指标的**总体均值**都是相等的。即 $\mu_1 = \mu_2 = ... = \mu_K$。这意味着影像学分组与临床表现无关。
                    *   $H_1$：至少有两个簇的临床指标的总体均值不相等。即 $\exists i, j$ 使得 $\mu_i \neq \mu_j$。这意味着影像学分组与临床表现显著相关。

                *   **检验方法的选择**:
                    代码首先通过**Shapiro-Wilk检验** 检查每个簇内临床数据的**正态性**。
                    *   $H_0^{\text{Shapiro}}$: 样本数据来自正态分布。
                    *   $H_1^{\text{Shapiro}}$: 样本数据不来自正态分布。

                    **情况A：如果所有组的数据都近似服从正态分布 (Shapiro检验的p值 > 0.05)**
                    此时采用**单因素方差分析 (ANOVA)**。ANOVA通过比较**组间方差**和**组内方差**来做出判断。其检验统计量 $F$ 的计算方式为：
                    $$ F = \frac{\text{MSB}}{\text{MSW}} = \frac{\sum_{k=1}^{K} N_k (\bar{y}_k - \bar{y})^2 / (K-1)}{\sum_{k=1}^{K} \sum_{i \in C_k} (y_i - \bar{y}_k)^2 / (N-K)} $$
                    其中 $\text{MSB}$ 是组间均方，$\text{MSW}$ 是组内均方，$\bar{y}$ 是所有样本的总均值，$N$ 是总样本数。若 $F$ 值很大，说明组间差异远大于组内差异，我们就有理由拒绝 $H_0$。

                    **情况B：如果至少有一组数据不服从正态分布**
                    此时采用非参数的**Kruskal-Wallis H检验**。该检验不依赖于数据分布的具体形式。它首先将所有样本的临床数据汇集在一起进行排序，然后计算每个组的**秩和**。检验统计量 $H$ 的计算公式为：
                    $$ H = \left( \frac{12}{N(N+1)} \sum_{k=1}^{K} \frac{R_k^2}{N_k} \right) - 3(N+1) $$
                    其中 $R_k$ 是第 $k$ 组的秩和。$H$ 统计量近似服从自由度为 $K-1$ 的卡方分布。

                *   **结论**: 无论使用哪种检验，最终都会得到一个**p值**。该p值表示“如果零假设$H_0$为真，我们观测到当前样本结果或更极端结果的概率”。如果p值小于一个预设的显著性水平 $\alpha$ (通常为0.05)，我们就拒绝$H_0$，得出结论：**新的影像学分组在临床指标上存在统计学上的显著差异**。

        *   **模式二：PC1驱动排序（如果没有临床数据）**:
            *   **原理**: 计算每个簇的**质心**在第一主成分（PC1）轴上的坐标。质心坐标 $\mu_{k, PC1}$ 的计算公式为：
              $$ \mu_{k, PC1} = \frac{1}{N_k} \sum_{\mathbf{z}_i \in C_k} z_{i,1} $$
              其中 $C_k$ 是第 $k$ 个簇， $N_k$ 是该簇的样本数，$\mathbf{z}_i$ 是降维后的第 $i$ 个样本的坐标向量，$z_{i,1}$ 是其在PC1轴上的值。然后根据 $\mu_{k, PC1}$ 的值对所有簇进行升序排序。

### 2.4 步骤四：确定最佳聚类数 (K)

*   **目标**：为层次聚类确定一个最合理的“切割”数 $K$，即最终分级的等级数量。
*   **方法一：层次聚类与树状图 (推荐)**: 树状图是对凝聚型层次聚类过程的完整可视化。其解释如下：
    *   **横轴**: 代表各个独立的样本点。
    *   **纵轴**: **核心是理解其含义**。在`method='ward'`的设定下，纵轴的高度并不直接表示两个簇之间的欧氏距离，而是代表了**合并两个簇所导致的“总簇内误差平方和 (WCSS)”的增加量**，即我们之前定义的合并成本 $\Delta(C_r, C_s)$。
        $$ \text{Height} = \Delta(C_r, C_s) = \text{WCSS}(C_r \cup C_s) - (\text{WCSS}(C_r) + \text{WCSS}(C_s)) $$
    *   **解读“最长的、未被切割的垂直线”**:
        树状图的结构是由一系列U形的“合并”连线组成的。每条垂直线段的长度，代表了从下一次合并（更细分的簇）到当前这次合并（更粗粒度的簇）之间，WCSS增加量的**差值**。
        一条**特别长**的垂直线意味着，为了将簇的数量从 $k+1$ 减少到 $k$，我们必须执行一次“代价高昂”的合并。这次合并的 $\Delta$ 值远大于之前所有的合并操作。
        这在数学上暗示，被合并的这两个簇在特征空间中相距很远，结构上非常不同，将它们强行合并会极大地增加数据的“总方差”或“混乱度”。因此，这个“代价高昂”的合并点是一个自然的分割点。我们应该在这个长垂直线被画出之前停止合并，即选择在这条长线下方进行“水平切割”。切割线穿过的垂直线条数，就是推荐的最佳聚类数 $K$。

*   **方法二：肘部法 (辅助参考)**
    *   **原理**: 该方法的核心是**簇内误差平方和 (WCSS)**。对于一个给定的聚类数 $K$，其WCSS计算公式为：
        $$ \text{WCSS}(K) = \sum_{k=1}^{K} \sum_{\mathbf{x}_i \in C_k} ||\mathbf{x}_i - \boldsymbol{\mu}_k||^2 $$
        其中，$C_k$ 是第 $k$ 个簇，$\mathbf{x}_i$ 是该簇中的一个样本点，$\boldsymbol{\mu}_k$ 是该簇的质心。
    *   **寻找拐点**: 肘部法通过量化WCSS随聚类数 $K$ 变化的趋势来寻找最佳 $K$。代码中采用的二阶差分法是定位“肘点”的一种自动化实现。

        1.  **WCSS序列**:
            首先，我们计算当聚类数 $K$ 取一系列连续整数（例如 $K=2, 3, ..., 10$）时对应的总簇内误差平方和WCSS。我们得到一个序列 $I = (I_2, I_3, ..., I_{10})$，其中 $I_K = \text{WCSS}(K)$。这个序列是单调递减的。

        2.  **一阶差分（模拟导数）**:
            为了找到下降速率变缓的点，我们首先计算序列 $I$ 的一阶差分，它代表了WCSS曲线的“斜率”。
            $$ \Delta I_K = I_{K+1} - I_K $$
            由于序列是递减的，$\Delta I_K$ 始终为负。它的绝对值$|\Delta I_K|$表示每增加一个簇，WCSS能减少多少。在“肘点”之前，$|\Delta I_K|$ 较大；在“肘点”之后，$|\Delta I_K|$ 变得很小。

        3.  **二阶差分（模拟曲率）**:
            “肘点”正是斜率变化最剧烈的地方。这种变化可以通过计算二阶差分来捕捉，它在离散序列中扮演了二阶导数的角色，可以衡量曲线的“弯曲程度”或曲率。
            $$ \Delta^2 I_K = \Delta I_{K+1} - \Delta I_K = (I_{K+2} - I_{K+1}) - (I_{K+1} - I_K) = I_{K+2} - 2I_{K+1} + I_K $$
            在一个理想的“肘形”曲线上，拐点处的二阶差分的绝对值$|\Delta^2 I_K|$会达到最大。

        4.  **确定拐点**:
            使二阶差分绝对值最大的那个点对应的聚类数 $K$，就被程序认为是最佳的“肘点”，并作为推荐的聚类数量。

---

## 3. 输入数据要求

为了实现“零配置”的自动化分析，请将所有输入文件放置在项目根目录下一个名为 `data/raw/` 的文件夹中。程序将根据以下命名和内容规则自动发现并处理它们。

### 3.1 原始特征总表 (必需)

这是主要数据文件，包含了所有样本在筛选前的全部影像组学特征，对应“一体化分析系统”的特征提取模块在未扰动的图像中提取的结果。

*   **作用**: 作为所有影像学特征的数据源。
*   **文件名**: 任意CSV文件名 (例如 `All_Features_2025.csv`)。
    *   **自动识别逻辑**: 程序会将其识别为 `data/raw/` 目录下**文件体积最大**的、且文件名中**不包含** "robust" 或 "clinical" 关键词的CSV文件。
*   **格式**:
    *   标准的CSV文件。
    *   **第一列**: 必须是**样本ID** (例如 `Sample_ID`, `Case_ID`, `id` 等)。程序会自动识别并将其作为合并数据的关键。
    *   **其余列**: 包含了所有从“一体化分析系统”中提取的、数值型的影像组学特征。
*   **特征命名规范**: 列名必须遵循“一体化分析系统”的输出规范 (e.g., `original_shape_Sphericity`, `dhi_dhi` 等)，以便程序能正确进行后续的多视角分组标准化。

### 3.2 稳健特征清单 (必需)

这是从“一体化分析系统”的稳健性相关性分析模块导出的文件，告诉本程序应该使用哪些特征进行分析。

*   **作用**: 作为特征筛选的“规则表”。
*   **文件名**: 文件名中必须包含 **`robust`** 关键词 (例如 `final_robust_features.csv`, `robust_feature_list.csv`)。
*   **格式**:
    *   标准的CSV文件。
    *   必须包含一个名为 **`feature`** 的列，该列中列出了所有通过稳健性测试的特征名称。这些名称必须与“原始特征总表”中的列名完全对应。

### 3.3 临床特征文件 (推荐)

如果希望将临床信息融入分级系统，请准备此文件。

*   **作用**: 为分级系统提供临床维度的验证和排序依据。
*   **文件名**: 文件名中必须包含 **`clinical`** 关键词 (例如 `clinical_data.csv`, `patient_clinical_info.csv`)。
*   **格式**:
    *   标准的CSV文件。
    *   **第一列**: **样本ID**。此ID列的名称和内容必须与“原始特征总表”中的ID完全对应，以便程序进行数据对齐（合并）。
    *   **其余列**: 数值型的临床特征（例如 `VAS_Score`, `ODI_Index` 等）。程序会自动识别所有数值列并忽略文本等非数值列。

如果程序在 `data/raw/` 目录下找到此文件，它会自动加载、对齐数据，并将临床特征智能地融入分级流程中。

---

## 4. 如何运行项目

1.  **环境设置**:
    *   确保已安装Anaconda（Python 3.9环境下已验证）。
    *   在项目根目录下，通过命令行安装所有必需的库：
        ```bash
        pip install -r requirements.txt
        ```

2.  **准备数据**:
    *   在项目根目录下创建一个名为 `data` 的文件夹，然后在其中再创建一个名为 `raw` 的文件夹 (最终路径为 `data/raw/`)。
    *   将上述准备好的 **2个必需文件**（原始特征总表、稳健特征清单）和 **1个可选文件**（临床数据）放入 `data/raw/` 文件夹中。

3.  **执行分析**:
    *   打开命令行（或终端），导航到项目根目录。

    *   **标准运行 (推荐)**:
        程序将自动发现文件，并默认使用 **PC1（数据内在结构）** 进行等级排序。
        ```bash
        python main.py
        ```

    *   **使用临床数据运行**:
        如果`data/raw/` 目录中包含了临床数据文件，使用此命令来启用**基于临床指标**的等级排序。
        ```bash
        python main.py --use-clinical
        ```
        *   还可以用 `--clinical-col "Your_Score_Name"` 来精确指定使用哪一列临床指标进行排序（默认是 `VAS_Score`）。

    *   **手动指定文件**:
        ```bash
        python main.py --raw-features "path/to/raw.csv" --robust-list "path/to/robust.csv" --clinical-data "path/to/clinical.csv"
        ```

    *   **交互式提示**:
        程序启动后，会进行一些简单的交互，让用户确认分析选项：
        *   `请选择聚类数量确定方式 (1. 自动 / 2. 手动):`

---

## 5\. 输出结果分析指南

程序运行成功后，会在 `results/` 目录下生成多个核心文件。请按照以下步骤对它们进行分析。

### **步骤一：确定等级数量**

*   **文件1**: `results/figures/dendrogram.png` (树状图)
    *   **分析 (定性)**: 观察图中**最长的、没有被横线切断的垂直线**，以此作为数据自然分组数量的**主要决策依据**。

*   **文件2**: `results/figures/elbow_plot.png` (肘部图)
    *   **分析 (定量)**: 寻找曲线的“肘点”，作为最佳聚类数的**参考建议**。

### **步骤二：验证分级效果 (看PCA散点图)**

*   **文件**: `results/figures/pca_scatter_plot.png`
*   **分析**:
    *   **看分离度**: 好的结果是，**相同颜色的点（代表同一个新等级）聚集在一起**，不同颜色的点清晰分开。
    *   **看趋势**: 观察不同等级（颜色）是否沿着**横轴PC1**呈现出有序的排列。

### **步骤三：建立新分级标准 (看特征与临床关联)**

*   **文件**: `results/tables/grade_interpretation.csv`**
*   **分析**:
    1.  **打开 `grade_interpretation.csv`**: 用Excel打开此文件，它展示了每个新等级在**关键影像组学特征**上的中位数。
    2.  **寻找趋势**: 观察关键特征（如 `shape_Sphericity`）的中位数值，是否随着 `New_Grade` 的递增或递减呈现出单调趋势。
    3.  **撰写标准**: 结合临床和影像学趋势，形成新的分级标准。例如：

| 新等级 | 临床表现 (来自命令行) | 影像组学特征描述 (来自CSV) | 可能的形态学解释 |
| :--- | :--- | :--- | :--- |
| **等级 1** | 平均疼痛评分最低 (e.g., 1.5 ± 0.5) | 球形度最高，信号强度四分位距最大。 | **健康/早期退变**：临床症状轻微，椎间盘形态饱满，含水量充足。 |
| ... | ... | ... | ... |
| **等级 4** | 平均疼痛评分最高 (e.g., 7.8 ± 1.2) | 球形度最低，信号强度四分位距最小。 | **重度退变**：临床症状严重，椎间盘形态严重失常，信号极低。 |

-----

## 6\. 参数设置说明

程序中的关键参数可以在 `config.py` 中进行调整，以适应不同的数据和分析需求：

*   `STANDARDIZATION_METHOD = 'robust'`: 指定对特征进行标准化的方法。
    *   **可选值**:
        *   `'robust'`: (默认) 使用中位数和四分位距进行缩放，对异常值不敏感，推荐用于小样本。
        *   `'zscore'`: 标准化为均值为0，标准差为1的正态分布。
        *   `'minmax'`: 将数据缩放到 `[0, 1]` 区间。

*   `PCA_N_COMPONENTS = 2`: PCA降维的目标维数。
    *   **效果**:
        *   `2`: (默认) 结果可以绘制为2D散点图，最常用。
        *   `3`: 结果可以绘制为3D散点图，提供更多视角。
        *   更高的值会保留更多原始数据方差，但无法直接可视化。

*   `CLUSTERING_METHOD = 'ward'`: 层次聚类的链接算法，即如何计算簇与簇之间的距离。
    *   **可选值**:
        *   `'ward'`: (默认) 旨在最小化簇内方差，倾向于产生大小相似的紧凑球形簇。**注意：此方法只支持欧氏距离**。
        *   `'complete'`: 完全链接，使用两簇中最远样本间的距离。
        *   `'average'`: 平均链接，使用两簇中所有样本对距离的平均值。
        *   `'single'`: 单链接，使用两簇中最近样本间的距离。

*   `CLUSTERING_METRIC = 'euclidean'`: 计算样本间距离的度量方式。
    *   **效果**: `'euclidean'` (默认) 即欧氏距离，是多维空间中的直线距离。除非有特殊理论支持，否则通常保持默认值。

*   `N_CLUSTERS = 8`: 当在程序交互提示中选择“手动指定”聚类数量时，所使用的**默认值**。

*   `DENDROGRAM_TRUNCATE_MODE = None`: 控制树状图的截断模式，用于简化大型树状图的可视化。
    *   **可选值**:
        *   `None`: (默认) 显示完整的树状图。
        *   `'lastp'`: 显示最后形成的 `p` 个簇。`p` 的值由 `DENDROGRAM_P` 定义。
        *   `'level'`: 仅显示指定深度之上的簇。

*   `DENDROGRAM_P = 30`: 当 `DENDROGRAM_TRUNCATE_MODE` 设置为 `'lastp'` 时，此参数指定要显示的簇的数量。

*   `CLINICAL_COLUMNS_CANDIDATES = ['Pain_Score', 'ODI_Score', 'VAS_Score', 'JOA_Score']`: 一个候选列表，程序会用这个列表中的列名去临床数据文件中匹配可用的临床指标。当启用临床排序但未通过命令行指定 `--clinical-col` 时，程序会使用在此列表中找到的第一个可用列进行排序。

## 7\. 参考文献
[1]Soydan Z, Bayramoglu E, Urut DU, Iplikcioglu AC, Sen C. Tracing the disc: The novel qualitative morphometric MRI based disc degeneration classification system. JOR Spine. 2024 Mar 18;7(1):e1321.

[2]McSweeney T, Tiulpin A, Kowlagi N, Määttä J, Karppinen J, Saarakkala S. Robust Radiomic Signatures of Intervertebral Disc Degeneration from MRI. Spine (Phila Pa 1976). 2025 Jun 20.

[3]Y. Gao, C. Cui, W. Zhang, and D. Han, Extended alternating structure-adapted proximal gradient algorithm for nonconvex nonsmooth problems, June 2024, arXiv:2406.17226.

[4]Perry, Ronan, et al. "mvlearn: Multiview Machine Learning in Python." Journal of Machine Learning Research 22.109 (2021): 1-7.

[5]R. Farias, J. Cohen, and P. Comon. Exploring multimodal data fusion through joint decompositions with flexible couplings. IEEE TSP, 64(18): 4830-4844, 2016.

[6]M. Nikolova and P. Tan. Alternating structure-adapted proximal gradient descent for nonconvex nonsmooth block-regularized problems. SIAM Journal on Optimization, 29(3): 2053–2078, 2019.

